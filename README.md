# GENAI-BIGDATA
생성형AI와 빅데이터분석 
---
# PART 1: 기초 다지기 (Weeks 1-3)  
## 1차시: 과정 소개 및 AI 시대의 빅데이터  
### 목표: 빅데이터의 개념과 AI와의 융합을 이해하고, 본 과정의 학습 목표와 필요성을 인식한다.

강의 내용:  
- 인공지능과 빅데이터 융합 시대 도래  
  - 빅데이터가 AI 학습에 어떻게 활용되는지 사례 설명 (예: 의료 진단 AI, 금융 리스크 예측)  
  - AI의 발전 역사 (규칙 기반 → 머신러닝 → 딥러닝 → 생성형 AI)와 현재 트렌드  
- 빅데이터의 5V 개념 상세 해설  
  - Volume: 데이터 양 증가 추세  
  - Velocity: 실시간 속도 데이터 처리 중요성  
  - Variety: 정형/비정형 데이터 종류와 특성  
  - Veracity: 데이터 신뢰성 문제  
  - Value: 실제 비즈니스 가치 창출 사례  
- 기업 및 산업별 빅데이터 활용 사례  
  - 금융권: 이상거래 탐지  
  - 제조업: 예측 유지보수  
  - 유통업: 고객 행동 분석  
  - 공공기관: 교통 흐름 예측  
- 본 과정 학습 로드맵과 기대 효과 안내  
- Q&A 및 간단 토론: AI 및 빅데이터가 본인 분야에 미칠 영향

---

## 2차시: 데이터 분석을 위한 개발 환경 구축  
### 목표: 데이터 분석에 필요한 개발 환경 설치 및 기본툴 사용법을 익혀, 바로 실습 가능하도록 준비한다.

강의 내용:  
- Anaconda 설치 및 가상환경 관리  
  - Anaconda란? 특징 및 필요성  
  - `conda` 명령어를 이용한 가상환경 생성 및 활성화/비활성화  
- Jupyter Notebook 실행 및 유용한 단축키 소개  
- VS Code 설치 및 Python 개발 환경 세팅 방법  
- GitHub 계정 생성 및 Git 기본 명령어 사용법  
  - 저장소 만들기(create repo) → Clone → Commit → Push → Pull  
- Google Colab 및 Kaggle Notebook 소개 및 활용법  
  - 클라우드 기반 실습 장점과 한계점 설명  
- 실습  
  - Anaconda 가상환경 만들기, Jupyter Notebook 실행  
  - GitHub에 본인 프로젝트 올리기 실습  
  - Google Colab에서 간단한 Python 코드 실행

---

## 3차시: Python 프로그래밍 핵심 기초  
### 목표: 데이터 분석 및 AI 구현에 필요한 Python 언어의 기본 문법과 구조를 익힌다.

강의 내용:  
- 기본 문법  
  - 변수 할당, 기본 자료형(정수, 실수, 문자열, 불린)  
  - 주석 처리 방법과 코딩 스타일(PEP8 간단 소개)  
- 제어문 소개  
  - 조건문: if-else 구문 작성법과 중첩 조건문  
  - 반복문: for 루프와 while 루프, break와 continue 활용법  
- 함수 정의와 호출  
  - `def` 키워드 이용 함수 정의  
  - 매개변수와 반환값  
  - 기본값 인자와 가변인자 (*args, kwargs) 소개  
- 클래스 및 객체지향 기초  
  - 클래스 정의 및 인스턴스 생성  
  - 초기화 메소드 `__init__`  
  - 속성(attribute)과 메서드(method) 개념  
- 예외 처리 (try-except) 기본 개념  
- 실습  
  - 조건문과 반복문을 사용한 구구단 출력 프로그램  
  - 리스트를 입력받아 평균 계산 함수 생성  
  - 간단한 BankAccount 클래스를 만들어 입출금 메서드 구현

---

## 4차시: 데이터 분석 라이브러리: NumPy  
### 목표: 고성능 수치 계산 라이브러리 NumPy를 활용해 다차원 배열을 이해하고, 효율적으로 데이터를 처리한다.

강의 내용:  
- NumPy 배열(ndarray) 생성과 기본 속성  
  - 리스트 대비 배열의 차이점과 성능 이점  
  - 배열 생성: `np.array()`, `np.zeros()`, `np.ones()`, `np.arange()`, `np.linspace()`  
  - 배열 차원, 크기(shape), 데이터 타입 확인 (`.shape`, `.ndim`, `.dtype`)  
- 배열 인덱싱과 슬라이싱  
  - 1차원 및 다차원 배열 인덱싱과 슬라이싱 기법  
  - 불리언 인덱싱과 팬시 인덱싱(fancy indexing) 원리  
- 배열 연산 및 브로드캐스팅  
  - 기본 사칙연산 및 수학 함수(`np.add`, `np.sqrt` 등) 적용  
  - 벡터화 연산과 for루프 대비 성능 비교  
  - 브로드캐스팅 규칙과 활용 사례  
- 배열 재구조화 및 변형  
  - `reshape()`, `flatten()`, `transpose()` 함수  
  - 배열 연결과 분할(`np.concatenate()`, `np.split()`)  
- 실습  
  - 2D 배열 생성 후 특정 행/열 슬라이싱  
  - 두 배열 덧셈 및 곱셈(벡터화)  
  - 브로드캐스팅을 이용한 행렬과 벡터 연산 예제

---

## 5차시: 데이터 분석 라이브러리: Pandas  
### 목표: 데이터 다루기의 핵심 라이브러리 Pandas를 익혀, 데이터프레임 조작 및 탐색적 데이터 분석(EDA)을 수행한다.

강의 내용:  
- Pandas 핵심 객체  
  - Series와 DataFrame의 구조 및 차이점 이해  
  - 데이터프레임 생성: 딕셔너리, 리스트, CSV 파일 불러오기(`read_csv()`)  
- 데이터 선택과 인덱싱  
  - `loc`, `iloc`를 이용한 행/열 단위 선택  
  - 조건 필터링 및 다중 조건 적용법  
- 데이터 입출력  
  - CSV, Excel, JSON 파일 읽고 쓰기  
  - SQL 쿼리를 통한 데이터 프레임 생성(기본 개념 소개)  
- 기초 탐색적 데이터 분석  
  - `head()`, `tail()`, `info()`, `describe()` 함수 활용  
  - 결측치 탐색(`isnull()`, `sum()`), 기초 통계 산출  
- 실습  
  - 공공 데이터셋 불러오기 및 기본 요약 통계 생성  
  - 특정 조건 필터링 예: 특정 연도, 지역 데이터 조회  
  - 엑셀 파일 저장 및 로드 체험

---

## 6차시: Pandas를 활용한 데이터 정제 및 변환  
### 목표: 실제 데이터 분석에서 반드시 필요한 데이터 클렌징 및 다양한 데이터 조작 기법을 익힌다.

강의 내용:  
- 결측치 처리 전략  
  - 결측치 탐색, 제거(`dropna()`), 대체(`fillna()`) 방법  
- 이상치 탐지와 처리 기법  
  - 사분위 수( IQR ) 이용 이상치 탐지  
  - 데이터 분포 시각화를 통한 이상점 식별  
- 데이터 그룹연산과 변환  
  - `groupby()`의 구조와 활용법  
  - 집계 함수 적용(`mean()`, `sum()`, `count()` 등)  
  - 피벗 테이블(`pivot_table()`)과 크로스탭(`crosstab()`) 활용  
- 데이터 병합과 연결  
  - `concat()`, `merge()`를 이용한 데이터 합치기  
  - 내부/외부 조인(inner, outer join)의 차이점  
- 실습  
  - 결측치 포함 데이터셋 정리 및 대체 실습  
  - 그룹화 후 특정 컬럼 평균 계산  
  - 두 개의 데이터프레임 병합 프로젝트

---

## 7차시: AI를 위한 핵심 선형대수  
### 목표: AI, 머신러닝 기반 알고리즘 이해에 필수적인 선형대수 기초 수학을 이해한다.

강의 내용:  
- 기초 개념  
  - 스칼라, 벡터, 행렬 정의 및 표현법  
  - 벡터와 행렬의 덧셈, 곱셈(내적/외적)  
- 행렬 연산 심화  
  - 행렬 곱 연산의 규칙과 성질  
  - 전치행렬과 대칭행렬  
  - 역행렬과 유사역행렬(pseudoinverse) 개념  
- 고유값과 고유벡터(Eigenvalue, Eigenvector)  
  - 정의 및 역할 설명 (선형변환의 축 방향)  
  - 실제 데이터에 대한 PCA 적용 기초 이론 연계  
- 특이값 분해 (SVD)  
  - SVD 분해 과정 및 활용 (차원 축소, 노이즈 제거)  
- 실습  
  - Numpy를 활용한 행렬 곱셈 및 역행렬 계산  
  - 간단한 고유값/고유벡터 직접 계산 실습

---

## 8차시: AI를 위한 핵심 확률 및 통계  
### 목표: AI 데이터 해석과 모델링의 기초가 되는 확률과 통계기법을 실무 예제와 함께 학습한다.

강의 내용:  
- 기술 통계학 복습  
  - 평균, 중앙값, 분산, 표준편차 계산법  
  - 산포도와 왜도, 첨도 기본 개념  
- 확률 기초  
  - 확률의 정의와 법칙(덧셈법칙, 곱셈법칙)  
  - 조건부 확률과 베이즈 정리  
- 확률분포 소개  
  - 주요 분포(이항, 정규, 포아송 등)와 성격  
  - 정규분포의 중요성 및 표준정규분포 변환  
- 중심극한정리 및 표본분포  
  - 표본평균의 분포 특성  
- 가설검정의 이해  
  - 귀무가설과 대립가설 설정  
  - 유의수준, p-값 의미  
  - t-검정, ANOVA 기본 원리  
- 실습  
  - 샘플 데이터로 기술통계 및 히스토그램 작성  
  - 실험 그룹간 평균 차이 t-검정 수행

---

## 9차시: 데이터 시각화  
### 목표: 데이터 탐색과 인사이트 도출을 위한 강력한 시각화 도구 Matplotlib, Seaborn, Plotly를 활용한다.

강의 내용:  
- Matplotlib 기본 함수  
  - 선 그래프, 막대 그래프, 히스토그램 그리기  
  - 축 레이블, 제목, 범례 배치법  
  - 다양한 스타일 및 색상 지정법  
- Seaborn 기반 시각화  
  - 통계적 그래프 종류: 박스플롯, 바이올린 플롯, 페어플롯 등  
  - 상관분석을 위한 히트맵 작성  
  - 카테고리형 데이터 시각화 방법  
- Plotly를 통한 인터랙티브 시각화  
  - 간단한 대화형 그래프 생성 및 웹 공유  
- 실습  
  - 공공 데이터셋으로 여러 유형 그래프 작성 및 평균, 분포 파악  
  - 페어플롯을 활용한 다변량 데이터 관계 시각화  
  - Plotly로 만든 대화형 차트 시연 및 구현


# PART 2: 핵심 기술 습득 (Weeks 4-7)

---

## 10차시: 데이터 수집 기술  
### 목표: 웹 스크레이핑 및 API를 활용한 데이터 수집 기술을 습득하고, SQL 고급 쿼리 활용 능력을 갖춘다.

강의 내용:  
- 웹 스크레이핑 개념 및 필요성  
  - HTML과 CSS 기본 구조 이해  
  - 웹 페이지 데이터 구조 분석 방법(웹개발자 도구 활용)  
- BeautifulSoup 활용법  
  - HTTP 요청 라이브러리(Requests) 사용법  
  - HTML 파싱, 태그 탐색 및 데이터 추출 방법  
- Selenium 활용법  
  - 동적 컨텐츠 크롤링 필요성  
  - 브라우저 자동화 실습(ChromeDriver 셋업, 페이지 이동)  
  - 로그인/페이지 스크롤 등 동적 요소 제어  
- API 데이터 수집  
  - RESTful API의 기본 원칙과 요청 방식(GET, POST 등)  
  - JSON 데이터 다루기 및 파싱  
  - 인증 토큰 활용 및 제한 정책 이해  
- SQL 고급 쿼리  
  - JOIN 구문의 종류(INNER, LEFT, RIGHT, FULL) 및 실습  
  - 서브쿼리 활용법과 성능 고려사항  
  - 집계 함수(GROUP BY, HAVING) 심화 학습  
- 실습  
  - 공공 데이터 포털에서 뉴스 데이터 API 호출하여 데이터 수집  
  - Selenium을 사용해 감성 리뷰 크롤링 후 저장  
  - Sample DB에서 JOIN문과 서브쿼리 활용해 복합 데이터 조회  

---

## 11차시: 고급 데이터 전처리 및 피처 엔지니어링  
### 목표: 데이터 품질 개선 및 머신러닝 입력값 최적화를 위한 전처리 및 변환 기법을 실습한다.

강의 내용:  
- 범주형 변수 인코딩  
  - 레이블 인코딩(LabelEncoding)과 원-핫 인코딩(One-Hot Encoding)의 차이 및 선택 기준  
  - 희소 행렬 문제와 해결책  
- 수치형 변수 스케일링  
  - 정규화(Normalization)와 표준화(Standardization) 방법과 실제 적용 예시  
  - Robust Scaler, MaxAbsScaler 등 다양한 스케일링 기법 개요  
- 차원 축소  
  - PCA 원리 및 설명요인(Explained Variance Ratio) 해석  
  - t-SNE와 UMAP을 이용한 비선형 차원 축소 시각화  
  - 차원 축소 시 주의점과 데이터 왜곡 방지법  
- 피처 엔지니어링  
  - 파생 변수 생성과 도메인 지식 적용  
  - 텍스트 데이터 기반 피처 만들기(텍스트 길이, 단어 빈도 등)  
- 실습  
  - Titanic 데이터셋에서 범주형 변수 인코딩과 누락값 처리  
  - PCA와 t-SNE를 통해 시각적 클러스터 확인  
  - 피처 조합을 통한 성능 향상 실험  

---

## 12차시: 머신러닝 기초: 지도학습  
### 목표: 대표적인 지도학습 모델의 개념과 수학적 배경을 이해하고, 직접 모델을 구현해본다.

강의 내용:  
- 선형 회귀(Linear Regression)  
  - 가설 함수, 비용 함수(Mean Squared Error) 정의  
  - 경사 하강법(Gradient Descent)으로 최적화  
  - Ridge, Lasso 회귀를 통한 과적합 방지성과 모델 선택  
- 로지스틱 회귀(Logistic Regression)  
  - 시그모이드 함수와 확률 해석  
  - 이진 분류 문제에 적용법과 비용함수(Cross Entropy)  
- 결정 트리(Decision Tree)  
  - 노드 분할 조건(지니 불순도, 엔트로피)  
  - 과적합 발생 원인과 가지치기(Pruning) 방법  
- 앙상블 모델  
  - 랜덤 포레스트(Random Forest)의 배깅(bagging) 원리  
  - 그래디언트 부스팅(Gradient Boosting)과 XGBoost, LightGBM 특징  
- 모델 평가 지표  
  - RMSE, MAE, R2 점수 해석 방법  
  - 혼동 행렬과 분류 정확도, 정밀도, 재현율, F1 점수  
- 실습  
  - sklearn 라이브러리 활용 Titanic 생존 예측 모델 구현  
  - 하이퍼파라미터 그리드 서치(GridSearchCV) 실습  

---

## 13차시: 머신러닝 기초: 비지도학습 및 평가  
### 목표: 군집화, 연관규칙 분석 등 비지도학습 개념과 적용법을 습득한다.

강의 내용:  
- 군집분석(Clustering)  
  - K-Means 알고리즘: 중심점 업데이트, 수렴 조건  
  - 군집 수 결정 방법(Elbow, Silhouette)  
  - DBSCAN: 밀도 기반 군집화, 노이즈 데이터 처리  
- 연관 규칙 분석  
  - Apriori 알고리즘 기본 원리  
  - 지지도(Support), 신뢰도(Confidence), 향상도(Lift) 개념  
  - 실생활 사례: 장바구니 분석, 마케팅 추천  
- 모델 평가지표 상세  
  - ROC 곡선과 AUC 설명  
  - Precision-Recall Curve 활용 시점  
- 실습  
  - 고객 구매 데이터 기반 K-Means 군집화 및 시각화  
  - 장바구니 데이터를 활용해 Apriori 규칙 추출과 해석  
  - 여러 평가 지표를 활용해 모델 성능 분석  

---

## 14차시: 딥러닝과 신경망의 이해  
### 목표: 신경망의 기본 구조와 학습원리를 이해하고, 기본 모델을 직접 구현한다.

강의 내용:  
- 퍼셉트론(Perceptron) 이해  
  - 단층 퍼셉트론과 XOR 문제의 한계  
- 다층 퍼셉트론(MLP)  
  - 은닉층 개념과 역할  
  - 활성화 함수 종류와 특징(ReLU, Sigmoid, Tanh)  
- 손실 함수  
  - 회귀용 MSE, 분류용 크로스 엔트로피 차이 설명  
- 최적화 기법: 경사 하강법  
  - 배치, 확률적, 미니배치 경사 하강법 구분  
- 역전파(Backpropagation) 알고리즘 원리  
  - 계산 그래프 및 사슬법칙(Chain Rule) 설명  
- 과적합 및 규제방법  
  - Dropout, L2 정규화 개념  
- 실습  
  - Numpy로 다층 퍼셉트론 구현  
  - 간단한 XOR 데이터를 MLP로 분류 실험  

---

네! PART 2(15~21차시)부터 PART 4(39차시)까지 각 차시별 상세 강의 내용을 이어서 작성해 드립니다.

---

# PART 2: 핵심 기술 습득 (계속)

---

## 15차시: 딥러닝 프레임워크 활용 (PyTorch/TensorFlow)  
### 목표: PyTorch 및 TensorFlow를 활용하여 신경망을 구축 및 학습하는 기본 프로세스를 익힌다.

강의 내용:  
- 텐서(Tensor) 소개 및 조작법  
  - Tensor 생성, reshape, 연산 기본  
  - GPU와 CPU 간 데이터 이동 방법  
- Autograd  
  - 자동 미분 기능과 계산 그래프 구조 이해  
  - backward() 함수 통한 경사 계산 및 변수 업데이트  
- 모델 구축  
  - Sequential 모델과 사용자 정의 Module 클래스  
  - 손실 함수, 옵티마이저 설정 및 학습 루프 구현  
- 평가 및 추론 모드 전환  
  - train()과 eval() 메서드 차이  
- 실습  
  - MNIST 데이터셋 분류 모델(PyTorch/TensorFlow) 구현 및 학습  
  - GPU 사용 확인 및 속도 비교  

---

## 16차시: 합성곱 신경망 (CNN)  
### 목표: CNN의 기본 구조와 원리를 익히고 이미지 분류 모델을 구현한다.

강의 내용:  
- 컨볼루션 레이어  
  - 필터, 스트라이드, 패딩의 작동 원리 및 시각적 이해  
- 풀링 레이어  
  - Max pooling과 평균 pooling 차이 및 효과  
- CNN 네트워크 구조  
  - 대표적 모델 VGG, ResNet의 특징 및 발전사  
- 전이 학습  
  - 사전학습 모델 불러와서 fine-tuning하는 방법  
- 실습  
  - CIFAR-10 이미지 분류용 CNN 모델 구현  
  - 사전학습 모델 ResNet18 불러와 커스터마이징

---

## 17차시: 순환 신경망 (RNN)  
### 목표: 시퀀스 데이터 처리용 RNN 모델 구조와 변종 LSTM/GRU의 개념을 익힌다.

강의 내용:  
- RNN 기본 구조  
  - 시퀀스 모델링 방식, 시간축 데이터 처리 설명  
- Vanishing Gradient 문제  
  - 문제 발생 원인과 학습 어려움  
- LSTM과 GRU  
  - 게이트 메커니즘과 내부 동작 방식 해설  
- 응용: 텍스트 생성 및 시퀀스 분류  
- 실습  
  - 간단한 RNN/LSTM 모델을 이용한 문자 생성 실험

---

## 18차시: 자연어 처리(NLP) 기초  
### 목표: 텍스트 데이터 전처리부터 단어 임베딩, 감성 분석 모델 구현까지 진행한다.

강의 내용:  
- 텍스트 전처리 기초  
  - 토큰화, 정제(소문자화, 특수문자 제거), 불용어 필터링  
- 단어 임베딩  
  - 원핫 인코딩 단점과 Word2Vec, GloVe 개념  
  - 임베딩 벡터 시각화(t-SNE)  
- 감성 분석 모델 제작  
  - Bag-of-Words, TF-IDF 벡터화 기법  
  - Logistic Regression, 간단한 LSTM 적용  
- 실습  
  - IMDB 영화 리뷰 데이터셋으로 감성 분류 모델 작성

---

## 19차시: 생성 모델의 원리: Autoencoder  
### 목표: 오토인코더의 구조 및 기능을 이해하고, 기본 구현법과 변종 모델을 학습한다.

강의 내용:  
- 오토인코더 개념  
  - 인코더-디코더 구조, 잠재 공간 학습  
- 잡음 제거 오토인코더(Denoising AE)  
  - 노이즈 제거를 통한 강건한 특징 추출  
- 변이형 오토인코더(VAE)  
  - 확률적 인코딩과 샘플링 개념  
  - ELBO 손실 함수 이해  
- 실습  
  - MNIST 데이터셋 기반 오토인코더 및 VAE 구현 및 시각화

---

## 20차시: 생성적 적대 신경망 (GAN)  
### 목표: GAN의 기본 원리와 학습 방법을 익히고, DCGAN을 직접 구현한다.

강의 내용:  
- GAN 구조  
  - 생성자(Generator)와 판별자(Discriminator) 역할 및 상호작용  
- 학습 절차  
  - 미니맥스 게임, 손실함수 수식적 이해  
- DCGAN  
  - 컨볼루션 기반 GAN 아키텍처  
- 학습 불안정성 문제 및 해결법  
  - 모드 붕괴, 불균형 학습 현상  
  - WGAN, Gradient Penalty 기법 소개  
- 실습  
  - DCGAN으로 수제작 이미지를 생성해 보기

---

## 21차시: Transformer와 Self-Attention 메커니즘  
### 목표: Transformer 아키텍처의 핵심인 Self-Attention을 이해하고 기본 모델을 구축한다.

강의 내용:  
- Seq2Seq 모델의 한계  
  - 반복구조 RNN 기반 문제 (병렬처리 한계 등)  
- Attention 메커니즘  
  - Query, Key, Value 개념과 점수 계산 방식  
- Self-Attention 원리  
  - 문장 내 단어 간 관계 학습  
- Transformer 구조  
  - Encoder와 Decoder 층별 구성  
  - 포지셔널 인코딩과 다중 어텐션(Multi-head Attention)  
- 실습  
  - 작은 Transformer 모형 구현 및 인코딩/디코딩 시연

---

# PART 3: 심화 및 응용 (Weeks 8-11)

---

## 22차시: 거대 언어 모델(LLM)의 이해  
### 목표: LLM의 학습 원리와 구조를 이해하고, API를 활용하여 간단한 텍스트 생성 모델을 구현한다.

강의 내용:  
- 사전학습(Pre-training)과 미세조정(Fine-tuning) 개념  
- GPT, BERT, T5 모델 구조와 역할 비교  
- LLM API 활용 방법  
  - OpenAI API, Hugging Face Transformers 설치 및 호출법  
  - 입력 프롬프트 작성법과 파라미터 조절  
- 실습  
  - API를 이용한 텍스트 생성, 완성 및 요약 실습

---

## 23차시: LLM 활용 기법: 프롬프트 엔지니어링  
### 목표: 다양한 프롬프트 작성 기법과 전략을 익히고, 생성형 AI 활용도를 높인다.

강의 내용:  
- 제로샷(Zeroshot) vs 퓨샷(Few-shot) 학습  
- Chain-of-Thought(생각의 연쇄) 프롬프트  
  - 문제를 단계별로 분해하여 해결 유도  
- 역할 기반 프롬프트(Role Prompting)  
  - AI에 특정 역할 지시로 효과 극대화  
- 프롬프트 디자인 패턴과 피해야 할 실수  
- 실습  
  - 다양한 샘플 프롬프트 작성 & 응답 비교 분석

---

## 24차시: LLM 활용 기법: RAG와 파인튜닝  
### 목표: 검색 증강 생성(RAG) 아키텍처와 파인튜닝 기법을 통해 맞춤형 LLM을 구축한다.

강의 내용:  
- RAG 개념과 작동 원리  
  - 외부 지식베이스 검색과 생성 모델 연동  
- 벡터 데이터베이스(FAISS, ChromaDB) 이해 및 구축  
  - 임베딩 생성 및 유사도 검색 논리  
- LoRA(저용량 파인튜닝) 원리 및 적용방법  
- 실습  
  - 도메인 데이터로 벡터 DB 구축 및 LLM 질문 응답 체계 구축

---

## 25차시: 멀티모달 생성 모델: Text-to-Image  
### 목표: 텍스트 조건 기반 이미지 생성 기술과 대표 모델들의 구조 및 활용법을 익힌다.

강의 내용:  
- 확산모델(Diffusion Model) 원리  
  - 노이즈 추가 및 제거 과정을 통한 생성  
- CLIP 모델  
  - 이미지와 텍스트 임베딩을 연결하는 멀티모달 학습  
- Stable Diffusion과 DALL-E2 아키텍처 소개  
- 실습  
  - 간단한 텍스트 프롬프트 입력, 이미지 생성 데모 실행  

---

## 26차시: 멀티모달 생성 모델: 그 외 응용  
### 목표: 이미지 캡셔닝, 음성/비디오 생성 등 멀티모달 모델 응용 사례를 이해한다.

강의 내용:  
- Image Captioning  
  - Encoder-Decoder 및 Transformer 기반 캡셔닝 모델  
- 텍스트 기반 음악, 비디오 생성 개념  
  - 최신 연구 동향 및 가능성  
- 멀티모달 임베딩 활용사례  
  - 크로스모달 검색, 콘텐츠 기반 추천 시스템  
- 실습  
  - Image Captioning 모델 기본 구현 및 테스트

---

## 27차시: 빅데이터 생태계의 이해  
### 목표: 빅데이터 처리 인프라 및 분산처리 기본 개념과 도구를 익힌다.

강의 내용:  
- 분산 컴퓨팅과 MapReduce 패러다임  
  - 문제 분할과 병렬처리 기본 아이디어  
- Hadoop 소개  
  - HDFS 파일 시스템과 YARN 자원관리  
- NoSQL DB 개요  
  - HBase, Cassandra 비교 및 활용 사례  
- 실습  
  - Hadoop 구성도 이해 및 Hadoop 데모 구동체험

---

## 28차시: 대용량 데이터 처리: Apache Spark  
### 목표: Spark의 핵심 개념과 PySpark를 활용한 대규모 데이터 분석 기술을 습득한다.

강의 내용:  
- Spark의 Driver와 Executor 역할  
- RDD와 DataFrame API 차이점 및 장단점  
- Spark SQL 쿼리 작성 및 실행  
- 실습  
  - PySpark로 차량 데이터 로드, 필터링, 집계 작업 수행

---

## 29차시: Spark를 활용한 머신러닝  
### 목표: Spark MLlib 활용법과 대용량 데이터 기반 추천시스템 제작 절차를 익힌다.

강의 내용:  
- Spark MLlib 주요 알고리즘 소개 (분류, 군집, 회귀)  
- ML 파이프라인 설계 및 Spark 사용 예  
- 추천 시스템 구성(협업 필터링, ALS)  
- 실습  
  - MovieLens 데이터셋으로 Spark 기반 추천 시스템 구현

---

## 30차시: 실시간 데이터 처리와 AI  
### 목표: Apache Kafka 및 Spark Streaming을 이용하여 실시간 데이터 스트림처리 기술을 익히고 AI적용 전략을 학습한다.

강의 내용:  
- 스트리밍 데이터 개념, Kafka 핵심 구성(Producer, Consumer, Broker)  
- Spark Structured Streaming 사용법  
- 실시간 이상 탐지 시스템 설계 및 적용 사례  
- 실습  
  - IoT 센서 데이터 스트리밍 수집 및 조건별 이상 감지

---

## 31차시: MLOps: 모델 배포 및 서빙  
### 목표: 모델 API 서버 구축과 배포 방법을 체험하여 실제 서비스화 기술을 익힌다.

강의 내용:  
- MLOps 개념과 필요성  
- FastAPI 기본 사용법  
- REST API 개발 및 모델 서빙 기술  
- Docker 컨테이너화 기초  
- 실습  
  - 학습한 모델 FastAPI 서버로 배포 및 테스트  
  - Docker 이미지 생성 및 컨테이너 실행

---

## 32차시: MLOps: CI/CD/CT 및 모니터링  
### 목표: 머신러닝 파이프라인 자동화와 시스템 모니터링 기법을 학습한다.

강의 내용:  
- CI/CD 파이프라인 소개 (GitHub Actions 예시)  
- 지속적 통합과 테스트(Continuous Testing) 자동화  
- 모델 모니터링과 데이터/모델 드리프트 탐지 기술  
- 실습  
  - GitHub 기반 자동화 파이프라인 구축 및 모니터링 대시보드 구성

---

## 33차시: MLOps 플랫폼 및 도구  
### 목표: MLflow, Kubeflow, 클라우드 MLOps 플랫폼 사용법과 특장점을 이해한다.

강의 내용:  
- MLflow: 실험 관리, 모델 레지스트리 핵심 기능  
- Kubeflow: 쿠버네티스 기반 ML 파이프라인 정의 및 실행  
- AWS SageMaker, GCP Vertex AI 비교 및 특징  
- 실습  
  - MLflow를 이용해 실험 추적 및 모델 버전 관리 진행

---

# PART 4: 프로젝트 및 최신 동향 (Weeks 12-13)

---

## 34차시: 최종 프로젝트 기획  
### 목표: 프로젝트 주제를 선정하고 구체적인 실행 계획과 목표를 수립한다.

강의 내용:  
- 주제 선정 시 고려사항과 기술 적합성 평가  
- 데이터 수집 및 분석 요구사항 정의  
- 기술 스택 및 아키텍처 설계  
- 일정 및 역할 분담 계획  
- 실습  
  - 프로젝트 계획서 작성 및 발표, 피드백 수렴

---

## 35차시: 최종 프로젝트 중간 점검 및 멘토링  
### 목표: 프로젝트 진행 상황 공유 및 문제점 해결을 위한 멘토링 진행.

강의 내용:  
- 데이터 전처리/EDA 결과 중간 점검  
- 초기 모델 성능 분석 및 병목 구간 탐색  
- 팀별 토론 및 멘토 개별 상담  
- 실습  
  - 중간 결과물 데모 및 개선 방향 토론

---

## 36차시: 모델 고도화 및 시스템 구현  
### 목표: 최종 모델 완성 및 시스템 연동, 성능 평가를 완료한다.

강의 내용:  
- 하이퍼파라미터 튜닝 기법 활용 (Grid/Random Search)  
- API 서버와 UI 통합 실습  
- 시스템 테스트 및 성능 평가 기준 마련  
- 실습  
  - 최적 모델 선정 및 엔드 투 엔드 통합 테스트

---

## 37차시: 최종 프로젝트 발표 및 평가  
### 목표: 프로젝트 결과를 발표하고, 동료 평가와 피드백을 통한 성찰 기회를 갖는다.

강의 내용:  
- 프로젝트 배경, 문제 정의 및 해결 전략 발표  
- 결과 시연 및 성능, 한계점 분석  
- 질의응답 및 토론  
- 평가 기준 설명 및 점수 부여  
- 실습  
  - 발표 자료 준비 및 발표 리허설

---

## 38차시: 생성형 AI와 빅데이터의 최신 동향  
### 목표: 최신 AI 연구 및 기술 트렌드, 미래 기술 동향 전반을 파악한다.

강의 내용:  
- AI 에이전트 및 자율 시스템 개념  
- 월드 모델과 시뮬레이션 기반 학습 동향  
- 양자컴퓨팅과 AI 융합 연구 소개  
- 산업별 최신 성공 사례 및 전망  
- 토론  
  - 미래 AI 활용 방안 및 진로 토론

---

## 39차시: AI 윤리와 미래 전망  
### 목표: AI 윤리 문제를 인식하고, 책임 있는 AI 개발과 사회적 영향력을 토론한다.

강의 내용:  
- 데이터 및 알고리즘 편향과 공정성 문제  
- 생성형 AI 확산에 따른 저작권 및 책임 문제  
- 사회적 책무감과 투명한 AI 개발 원칙  
- 향후 AI 기술 발전에 따른 법률 및 정책 변화 전망  
- 실습  
  - AI 윤리 사례 연구 및 토론  
  - 커리어 로드맵 및 지속 학습 전략 수립

---

